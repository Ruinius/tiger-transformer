{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import os\n",
                "import glob\n",
                "import numpy as np"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Config\n",
                "BS_CLEAN_DIR = '../balance_sheet_clean_label'\n",
                "IS_CLEAN_DIR = '../income_statement_clean_label'\n",
                "BS_MAPPING = 'bs_calculated_operating_mapping.csv'\n",
                "IS_MAPPING = 'is_calculated_operating_expense_mapping.csv'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Successfully sorted 'is_calculated_operating_expense_mapping.csv' by 'avg_order_fwd'\n",
                        "     standardized_name  frequency  avg_order_fwd  avg_order_rev  \\\n",
                        "0              revenue         29           1.00          13.72   \n",
                        "1        gross_revenue          2           2.00          19.00   \n",
                        "2      segment_revenue         78           2.46          17.51   \n",
                        "3  revenue_adjustments          2           3.00          18.00   \n",
                        "4      sales_discounts          2           4.00          17.00   \n",
                        "\n",
                        "   is_calculated  is_operating is_expense  \n",
                        "0          False          True      False  \n",
                        "1          False          True      False  \n",
                        "2          False          True      False  \n",
                        "3          False          True       True  \n",
                        "4          False          True       True  \n"
                    ]
                }
            ],
            "source": [
                "# --- Sort and Save Existing CSV ---\n",
                "# Parameters\n",
                "SORT_COLUMN = 'avg_order_fwd'\n",
                "# SORT_COLUMN = 'avg_order_rev'\n",
                "# SORT_COLUMN = 'frequency'\n",
                "ASCENDING = True            # Set to False for descending order\n",
                "FILE_PATH = IS_MAPPING     # Uses the variable from the config cell\n",
                "\n",
                "try:\n",
                "    if os.path.exists(FILE_PATH):\n",
                "        df_mapping = pd.read_csv(FILE_PATH)\n",
                "        \n",
                "        # Check if column exists\n",
                "        if SORT_COLUMN in df_mapping.columns:\n",
                "            # Sort\n",
                "            df_mapping = df_mapping.sort_values(by=SORT_COLUMN, ascending=ASCENDING)\n",
                "            \n",
                "            # Save\n",
                "            df_mapping.to_csv(FILE_PATH, index=False)\n",
                "            print(f\"Successfully sorted '{FILE_PATH}' by '{SORT_COLUMN}'\")\n",
                "            print(df_mapping.head())\n",
                "        else:\n",
                "            print(f\"Error: Column '{SORT_COLUMN}' not found in {FILE_PATH}\")\n",
                "            print(f\"Available columns: {list(df_mapping.columns)}\")\n",
                "    else:\n",
                "        print(f\"Error: File '{FILE_PATH}' does not exist.\")\n",
                "except Exception as e:\n",
                "    print(f\"An error occurred: {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loaded existing mapping with 54 rows.\n",
                        "Reading 60 files from ../income_statement_clean_label...\n",
                        "Success! Updated is_calculated_operating_expense_mapping.csv with 54 rows.\n",
                        "      standardized_name  frequency  avg_order_fwd  avg_order_rev  \\\n",
                        "41              revenue         29           1.00          13.72   \n",
                        "15        gross_revenue          2           2.00          19.00   \n",
                        "51      segment_revenue         78           2.46          17.51   \n",
                        "42  revenue_adjustments          2           3.00          18.00   \n",
                        "45      sales_discounts          2           4.00          17.00   \n",
                        "\n",
                        "    is_calculated is_operating is_expense  \n",
                        "41          False         True      False  \n",
                        "15          False         True      False  \n",
                        "51          False         True      False  \n",
                        "42          False         True       True  \n",
                        "45          False         True       True  \n"
                    ]
                }
            ],
            "source": [
                "# Parameters\n",
                "OUTPUT_FILE = IS_MAPPING     # Use BS_MAPPING or IS_MAPPING based on what you are processing\n",
                "CLEAN_DIR = IS_CLEAN_DIR     # Use BS_CLEAN_DIR or IS_CLEAN_DIR\n",
                "# 1. Load Existing Mapping (to preserve manual work)\n",
                "if os.path.exists(OUTPUT_FILE):\n",
                "    existing_df = pd.read_csv(OUTPUT_FILE)\n",
                "    print(f\"Loaded existing mapping with {len(existing_df)} rows.\")\n",
                "else:\n",
                "    existing_df = pd.DataFrame()\n",
                "    print(\"No existing mapping found. Creating new.\")\n",
                "# 2. Recalculate Stats from Clean Data\n",
                "files = glob.glob(os.path.join(CLEAN_DIR, \"*.csv\"))\n",
                "df_list = []\n",
                "print(f\"Reading {len(files)} files from {CLEAN_DIR}...\")\n",
                "for f in files:\n",
                "    try:\n",
                "        d = pd.read_csv(f)\n",
                "        df_list.append(d)\n",
                "    except Exception as e:\n",
                "        print(f\"Error reading {f}: {e}\")\n",
                "if not df_list:\n",
                "    raise ValueError(\"No CSV files found.\")\n",
                "combined_df = pd.concat(df_list, ignore_index=True)\n",
                "# Calculate Orders\n",
                "combined_df['fwd_order'] = combined_df.groupby('company').cumcount() + 1\n",
                "combined_df['co_len'] = combined_df.groupby('company')['standardized_name'].transform('count')\n",
                "combined_df['rev_order'] = combined_df['co_len'] - combined_df['fwd_order'] + 1\n",
                "# Clean is_calculated\n",
                "if 'is_calculated' in combined_df.columns:\n",
                "    combined_df['is_calculated'] = combined_df['is_calculated'].replace(\n",
                "        {'True': True, 'False': False, 'TRUE': True, 'FALSE': False}\n",
                "    )\n",
                "def get_consistent_calculated(series):\n",
                "    uniques = series.dropna().unique()\n",
                "    if len(uniques) == 1:\n",
                "        return uniques[0]\n",
                "    return np.nan\n",
                "# Aggregate\n",
                "new_stats = combined_df.groupby('standardized_name').agg(\n",
                "    frequency=('standardized_name', 'count'),\n",
                "    avg_order_fwd=('fwd_order', 'mean'),\n",
                "    avg_order_rev=('rev_order', 'mean'),\n",
                "    new_is_calculated=('is_calculated', get_consistent_calculated)\n",
                ").reset_index()\n",
                "# Rounding\n",
                "new_stats['avg_order_fwd'] = new_stats['avg_order_fwd'].round(2)\n",
                "new_stats['avg_order_rev'] = new_stats['avg_order_rev'].round(2)\n",
                "# 3. Merge and Update\n",
                "# We want to keep all rows from new_stats (in case there are new standardized names)\n",
                "# We want to preserve manual columns from existing_df if they exist\n",
                "if not existing_df.empty:\n",
                "    # Merge on standardized_name\n",
                "    merged = pd.merge(new_stats, existing_df, on='standardized_name', how='left', suffixes=('', '_old'))\n",
                "    \n",
                "    # Update is_calculated logic:\n",
                "    # If the user manually edited it, we might want to keep it? \n",
                "    # Your request says \"fill ... with content in the csv\", implying we overwrite Calculated \n",
                "    # but keep Operating.\n",
                "    # Let's overwrite is_calculated with the newly derived one (since it comes from the clean labels)\n",
                "    merged['is_calculated'] = merged['new_is_calculated']\n",
                "    \n",
                "    # Preserve is_operating (and is_expense if it exists)\n",
                "    if 'is_operating' not in merged.columns:\n",
                "        merged['is_operating'] = np.nan\n",
                "    \n",
                "    # If specifically processing Income Statement, ensure is_expense exists\n",
                "    if 'is_expense' not in merged.columns and 'is_expense' in existing_df.columns:\n",
                "         merged['is_expense'] = existing_df['is_expense'] \n",
                "    elif 'is_expense' not in merged.columns:\n",
                "         merged['is_expense'] = np.nan\n",
                "    # Select and Clean Columns\n",
                "    cols = ['standardized_name', 'frequency', 'avg_order_fwd', 'avg_order_rev', 'is_calculated', 'is_operating']\n",
                "    if 'is_expense' in merged.columns:\n",
                "        cols.append('is_expense')\n",
                "        \n",
                "    final_df = merged[cols]\n",
                "else:\n",
                "    # First run\n",
                "    final_df = new_stats.rename(columns={'new_is_calculated': 'is_calculated'})\n",
                "    final_df['is_operating'] = np.nan\n",
                "\n",
                "# Check if standardized_name column exists and filter\n",
                "if 'standardized_name' in final_df.columns:\n",
                "    final_df = final_df[~final_df['standardized_name'].str.endswith('_header')]\n",
                "\n",
                "# 4. Save\n",
                "# Sort by frequency or whatever your preference is before saving\n",
                "final_df = final_df.sort_values(by=['avg_order_fwd'])\n",
                "final_df.to_csv(OUTPUT_FILE, index=False)\n",
                "print(f\"Success! Updated {OUTPUT_FILE} with {len(final_df)} rows.\")\n",
                "print(final_df.head())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loaded existing mapping with 104 rows.\n",
                        "Reading 53 files from ../balance_sheet_clean_label...\n",
                        "Success! Updated bs_calculated_operating_mapping.csv with 104 rows.\n",
                        "             standardized_name  frequency  avg_order_fwd  avg_order_rev  \\\n",
                        "13        cash_and_equivalents         53           1.32          29.91   \n",
                        "88             restricted_cash          8           2.38          36.75   \n",
                        "95      short_term_investments         37           2.70          30.95   \n",
                        "1          accounts_receivable         44           3.45          28.73   \n",
                        "99  total_cash_and_investments          4           3.50          29.25   \n",
                        "\n",
                        "    is_calculated is_operating   nonoperating_category  \n",
                        "13          False        False                    cash  \n",
                        "88          False        False                    cash  \n",
                        "95          False        False  short_term_investments  \n",
                        "1           False         True                     NaN  \n",
                        "99           True        False                     NaN  \n"
                    ]
                }
            ],
            "source": [
                "# Parameters\n",
                "OUTPUT_FILE = BS_MAPPING     # Use BS_MAPPING or IS_MAPPING based on what you are processing\n",
                "CLEAN_DIR = BS_CLEAN_DIR     # Use BS_CLEAN_DIR or IS_CLEAN_DIR\n",
                "# 1. Load Existing Mapping (to preserve manual work)\n",
                "if os.path.exists(OUTPUT_FILE):\n",
                "    existing_df = pd.read_csv(OUTPUT_FILE)\n",
                "    print(f\"Loaded existing mapping with {len(existing_df)} rows.\")\n",
                "else:\n",
                "    existing_df = pd.DataFrame()\n",
                "    print(\"No existing mapping found. Creating new.\")\n",
                "# 2. Recalculate Stats from Clean Data\n",
                "files = glob.glob(os.path.join(CLEAN_DIR, \"*.csv\"))\n",
                "df_list = []\n",
                "print(f\"Reading {len(files)} files from {CLEAN_DIR}...\")\n",
                "for f in files:\n",
                "    try:\n",
                "        d = pd.read_csv(f)\n",
                "        df_list.append(d)\n",
                "    except Exception as e:\n",
                "        print(f\"Error reading {f}: {e}\")\n",
                "if not df_list:\n",
                "    raise ValueError(\"No CSV files found.\")\n",
                "combined_df = pd.concat(df_list, ignore_index=True)\n",
                "# Calculate Orders\n",
                "combined_df['fwd_order'] = combined_df.groupby('company').cumcount() + 1\n",
                "combined_df['co_len'] = combined_df.groupby('company')['standardized_name'].transform('count')\n",
                "combined_df['rev_order'] = combined_df['co_len'] - combined_df['fwd_order'] + 1\n",
                "# Clean is_calculated\n",
                "if 'is_calculated' in combined_df.columns:\n",
                "    combined_df['is_calculated'] = combined_df['is_calculated'].replace(\n",
                "        {'True': True, 'False': False, 'TRUE': True, 'FALSE': False}\n",
                "    )\n",
                "def get_consistent_calculated(series):\n",
                "    uniques = series.dropna().unique()\n",
                "    if len(uniques) == 1:\n",
                "        return uniques[0]\n",
                "    return np.nan\n",
                "# Aggregate\n",
                "new_stats = combined_df.groupby('standardized_name').agg(\n",
                "    frequency=('standardized_name', 'count'),\n",
                "    avg_order_fwd=('fwd_order', 'mean'),\n",
                "    avg_order_rev=('rev_order', 'mean'),\n",
                "    new_is_calculated=('is_calculated', get_consistent_calculated)\n",
                ").reset_index()\n",
                "# Rounding\n",
                "new_stats['avg_order_fwd'] = new_stats['avg_order_fwd'].round(2)\n",
                "new_stats['avg_order_rev'] = new_stats['avg_order_rev'].round(2)\n",
                "# 3. Merge and Update\n",
                "# We want to keep all rows from new_stats (in case there are new standardized names)\n",
                "# We want to preserve manual columns from existing_df if they exist\n",
                "if not existing_df.empty:\n",
                "    # Merge on standardized_name\n",
                "    merged = pd.merge(new_stats, existing_df, on='standardized_name', how='left', suffixes=('', '_old'))\n",
                "    \n",
                "    # Update is_calculated logic:\n",
                "    # If the user manually edited it, we might want to keep it? \n",
                "    # Your request says \"fill ... with content in the csv\", implying we overwrite Calculated \n",
                "    # but keep Operating.\n",
                "    # Let's overwrite is_calculated with the newly derived one (since it comes from the clean labels)\n",
                "    merged['is_calculated'] = merged['new_is_calculated']\n",
                "    \n",
                "    # Preserve is_operating (and is_expense if it exists)\n",
                "    if 'is_operating' not in merged.columns:\n",
                "        merged['is_operating'] = np.nan\n",
                "    \n",
                "    # If specifically processing Balance Sheet, ensure nonoperating_category exists\n",
                "    if 'nonoperating_category' not in merged.columns and 'nonoperating_category' in existing_df.columns:\n",
                "         merged['nonoperating_category'] = existing_df['nonoperating_category'] \n",
                "    elif 'nonoperating_category' not in merged.columns:\n",
                "         merged['nonoperating_category'] = np.nan\n",
                "    # Select and Clean Columns\n",
                "    cols = ['standardized_name', 'frequency', 'avg_order_fwd', 'avg_order_rev', 'is_calculated', 'is_operating']\n",
                "    if 'nonoperating_category' in merged.columns:\n",
                "        cols.append('nonoperating_category')\n",
                "        \n",
                "    final_df = merged[cols]\n",
                "else:\n",
                "    # First run\n",
                "    final_df = new_stats.rename(columns={'new_is_calculated': 'is_calculated'})\n",
                "    final_df['nonoperating_category'] = np.nan\n",
                "    \n",
                "# Check if standardized_name column exists and filter\n",
                "if 'standardized_name' in final_df.columns:\n",
                "    final_df = final_df[~final_df['standardized_name'].str.endswith('_header')]\n",
                "\n",
                "# 4. Save\n",
                "# Sort by frequency or whatever your preference is before saving\n",
                "final_df = final_df.sort_values(by=['avg_order_fwd'])\n",
                "final_df.to_csv(OUTPUT_FILE, index=False)\n",
                "print(f\"Success! Updated {OUTPUT_FILE} with {len(final_df)} rows.\")\n",
                "print(final_df.head())"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "myaiml",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
